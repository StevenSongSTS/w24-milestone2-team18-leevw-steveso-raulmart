{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Steven\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Steven\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Steven\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "c:\\Users\\Steven\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Steven\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from constants import SHARED_RANDOM_STATE\n",
    "from db_helper_functions import get_stock_news_from_db\n",
    "from text_cleaning_functions import clean_text\n",
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from finbert_embedding.embedding import FinbertEmbedding\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "import torch\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from db\n",
    "df = get_stock_news_from_db(\"AAPL\")\n",
    "df = df[~df.article.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial cleaning\n",
    "df[\"article\"] = df[\"article\"].apply(\n",
    "    lambda x: x.replace(\"\\xa0\", \" \").replace(\"\\n\", \"\").replace(\"Loading...\", \"\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize vectorizer\n",
    "tf_vectorizer = CountVectorizer(stop_words=\"english\", ngram_range=(1, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def calculate_article_score(top_distribution, score_array):\n",
    "    product = [a * b for a, b in zip(top_distribution, score_array)]\n",
    "    sum_of_products = sum(product)\n",
    "    \n",
    "    return sum_of_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calcualte the mean sentiment score\n",
    "\n",
    "def get_mean_sentiment_score(df, vectorizer, num_topic):\n",
    "    docs = list(df[\"article\"])\n",
    "\n",
    "    model = BERTopic(\n",
    "        vectorizer_model=vectorizer,\n",
    "        language=\"english\",\n",
    "        calculate_probabilities=True,\n",
    "        nr_topics=num_topic,\n",
    "    )\n",
    "    \n",
    "    topic_model = model.fit(docs)\n",
    "    topic_distr, _ = topic_model.approximate_distribution(docs)\n",
    "\n",
    "    # Get the semantic score for each topic\n",
    "    finbert_tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "    finbert_model = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\")\n",
    "\n",
    "    # Get topic info from the topic model\n",
    "    topic_info_df = topic_model.get_topic_info()\n",
    "\n",
    "    # Get number of topics \n",
    "    topic_num = len(topic_info_df) \n",
    "\n",
    "    # Start with 1 because topic -1 are outliers\n",
    "    for i in range(1,topic_num):\n",
    "\n",
    "        print(f\"processing topic{i}\")\n",
    "        # Get representative documents\n",
    "        representative_docs = topic_info_df.loc[i][\"Representative_Docs\"]\n",
    "\n",
    "        # Tokenize the articles into sentences\n",
    "        representative_docs_sentences = [sent_tokenize(x) for x in representative_docs]\n",
    "\n",
    "        # Initiate empty array for the sentiment score of each representitive document\n",
    "        representative_docs_score= []\n",
    "\n",
    "        # Traverse each document and calculate the score \n",
    "        for sentence_arr in representative_docs_sentences:\n",
    "            if len(sentence_arr) > 100:\n",
    "                sentence_arr = sentence_arr[:100]\n",
    "            #print(f\"The length of the sentence array is{len(sentence_arr)}\")\n",
    "            embedding = finbert_tokenizer(\n",
    "                    sentence_arr, padding=True, return_tensors=\"pt\", max_length=512, truncation=True\n",
    "                )\n",
    "            outputs = finbert_model(**embedding)\n",
    "            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "\n",
    "            score = np.round(np.mean(predictions.tolist(), axis=0), 4)\n",
    "            representative_docs_score.append(list(score))\n",
    "\n",
    "        representative_docs_score = np.array(representative_docs_score)\n",
    "        mean_value_list = np.mean(representative_docs_score, axis=0)\n",
    "\n",
    "        # Add the semantic score to the topic dataframe\n",
    "        topic_info_df.loc[i,\"positive\"] = mean_value_list[0]\n",
    "        topic_info_df.loc[i,\"negative\"] = mean_value_list[1]\n",
    "        topic_info_df.loc[i,\"neutral\"] = mean_value_list[2]\n",
    "    \n",
    "    # Calculate the semantic score for each articles\n",
    "    topics_positive_score = topic_info_df[\"positive\"][1:].tolist()\n",
    "    topics_negative_score = topic_info_df[\"negative\"][1:].tolist()\n",
    "    topics_neutral_score = topic_info_df[\"neutral\"][1:].tolist()\n",
    "\n",
    "    article_score_array = []\n",
    "\n",
    "    for article_topic_distribution in topic_distr:\n",
    "        article_positive = calculate_article_score(article_topic_distribution, topics_positive_score)\n",
    "        article_negative = calculate_article_score(article_topic_distribution, topics_negative_score)\n",
    "        article_neutral = calculate_article_score(article_topic_distribution, topics_neutral_score)\n",
    "\n",
    "        article_score_array.append([article_positive, article_negative,  article_neutral])\n",
    "\n",
    "    df[[\"positive\", \"negative\", \"neutral\"]] = article_score_array\n",
    "\n",
    "    positive_mean = df[\"positive\"].mean()\n",
    "    negative_mean = df[\"negative\"].mean()\n",
    "    neutral_mean = df[\"neutral\"].mean()\n",
    "\n",
    "    return (positive_mean,negative_mean,neutral_mean)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start testing with 5 topics\n",
      "processing topic1\n",
      "processing topic2\n",
      "processing topic3\n",
      "processing topic4\n",
      "Start testing with 10 topics\n",
      "processing topic1\n",
      "processing topic2\n",
      "processing topic3\n",
      "processing topic4\n",
      "processing topic5\n",
      "processing topic6\n",
      "processing topic7\n",
      "processing topic8\n",
      "processing topic9\n",
      "Start testing with 15 topics\n",
      "processing topic1\n",
      "processing topic2\n",
      "processing topic3\n",
      "processing topic4\n",
      "processing topic5\n",
      "processing topic6\n",
      "processing topic7\n",
      "processing topic8\n",
      "processing topic9\n",
      "processing topic10\n",
      "processing topic11\n",
      "processing topic12\n",
      "processing topic13\n",
      "processing topic14\n",
      "Start testing with 20 topics\n",
      "processing topic1\n",
      "processing topic2\n",
      "processing topic3\n",
      "processing topic4\n",
      "processing topic5\n",
      "processing topic6\n",
      "processing topic7\n",
      "processing topic8\n",
      "processing topic9\n",
      "processing topic10\n",
      "processing topic11\n",
      "processing topic12\n",
      "processing topic13\n",
      "processing topic14\n",
      "processing topic15\n",
      "processing topic16\n",
      "processing topic17\n",
      "processing topic18\n",
      "processing topic19\n",
      "Start testing with 25 topics\n",
      "processing topic1\n",
      "processing topic2\n",
      "processing topic3\n",
      "processing topic4\n",
      "processing topic5\n",
      "processing topic6\n",
      "processing topic7\n",
      "processing topic8\n",
      "processing topic9\n",
      "processing topic10\n",
      "processing topic11\n",
      "processing topic12\n",
      "processing topic13\n",
      "processing topic14\n",
      "processing topic15\n",
      "processing topic16\n",
      "processing topic17\n",
      "processing topic18\n",
      "processing topic19\n",
      "processing topic20\n",
      "processing topic21\n",
      "processing topic22\n",
      "processing topic23\n",
      "processing topic24\n",
      "Start testing with 30 topics\n",
      "processing topic1\n",
      "processing topic2\n",
      "processing topic3\n",
      "processing topic4\n",
      "processing topic5\n",
      "processing topic6\n",
      "processing topic7\n",
      "processing topic8\n",
      "processing topic9\n",
      "processing topic10\n",
      "processing topic11\n",
      "processing topic12\n",
      "processing topic13\n",
      "processing topic14\n",
      "processing topic15\n",
      "processing topic16\n",
      "processing topic17\n",
      "processing topic18\n",
      "processing topic19\n",
      "processing topic20\n",
      "processing topic21\n",
      "processing topic22\n",
      "processing topic23\n",
      "processing topic24\n",
      "processing topic25\n",
      "processing topic26\n",
      "processing topic27\n",
      "processing topic28\n",
      "processing topic29\n",
      "Start testing with 35 topics\n",
      "processing topic1\n",
      "processing topic2\n",
      "processing topic3\n",
      "processing topic4\n",
      "processing topic5\n",
      "processing topic6\n",
      "processing topic7\n",
      "processing topic8\n",
      "processing topic9\n",
      "processing topic10\n",
      "processing topic11\n",
      "processing topic12\n",
      "processing topic13\n",
      "processing topic14\n",
      "processing topic15\n",
      "processing topic16\n",
      "processing topic17\n",
      "processing topic18\n",
      "processing topic19\n",
      "processing topic20\n",
      "processing topic21\n",
      "processing topic22\n",
      "processing topic23\n",
      "processing topic24\n",
      "processing topic25\n",
      "processing topic26\n",
      "processing topic27\n",
      "processing topic28\n",
      "processing topic29\n",
      "processing topic30\n",
      "processing topic31\n",
      "processing topic32\n",
      "processing topic33\n",
      "processing topic34\n",
      "Start testing with 40 topics\n",
      "processing topic1\n",
      "processing topic2\n",
      "processing topic3\n",
      "processing topic4\n",
      "processing topic5\n",
      "processing topic6\n",
      "processing topic7\n",
      "processing topic8\n",
      "processing topic9\n",
      "processing topic10\n",
      "processing topic11\n",
      "processing topic12\n",
      "processing topic13\n",
      "processing topic14\n",
      "processing topic15\n",
      "processing topic16\n",
      "processing topic17\n",
      "processing topic18\n",
      "processing topic19\n",
      "processing topic20\n",
      "processing topic21\n",
      "processing topic22\n",
      "processing topic23\n",
      "processing topic24\n",
      "processing topic25\n",
      "processing topic26\n",
      "processing topic27\n",
      "processing topic28\n",
      "processing topic29\n",
      "processing topic30\n",
      "processing topic31\n",
      "processing topic32\n",
      "processing topic33\n",
      "processing topic34\n",
      "processing topic35\n",
      "processing topic36\n",
      "processing topic37\n",
      "processing topic38\n",
      "processing topic39\n"
     ]
    }
   ],
   "source": [
    "topic_num_arr = [5, 10, 15, 20, 25, 30, 35, 40]\n",
    "result_df = pd.DataFrame(columns=[\"number_of_topic\",\"mean_positive_score\", \"mean_negative_score\", \"mean_neutral_score\"])\n",
    "\n",
    "for topic_num in topic_num_arr:\n",
    "    print(f\"Start testing with {topic_num} topics\")\n",
    "    positive_mean, negative_mean, neutral_mean = get_mean_sentiment_score(df, tf_vectorizer, topic_num)\n",
    "\n",
    "    new_row = {\n",
    "        \"number_of_topic\":topic_num,\n",
    "        \"mean_positive_score\":positive_mean,\n",
    "        \"mean_negative_score\":negative_mean,\n",
    "        \"mean_neutral_score\":neutral_mean,\n",
    "    }\n",
    "\n",
    "    result_df = result_df._append(new_row, ignore_index=True)\n",
    "    time.sleep(60)\n",
    "\n",
    "# Write result to csv\n",
    "result_df.to_csv('sensitivity_analysis.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
